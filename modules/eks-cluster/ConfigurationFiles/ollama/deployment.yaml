apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: 891612581521.dkr.ecr.us-east-1.amazonaws.com/private-ai-deployment-ci-cd/ollama:v10
          imagePullPolicy: Always
          ports:
            - containerPort: 11434
          env:
            - name: OLLAMA_NUM_PARALLEL
              value: "8"
            - name: OLLAMA_MAX_LOADED_MODELS
              value: "4"
          resources:
            limits:
              nvidia.com/gpu: 4
              ephemeral-storage: "20Gi"
            requests:
              nvidia.com/gpu: 4
              ephemeral-storage: "10Gi"
          volumeMounts:
            - mountPath: /root/.ollama/models
              name: ollama-models
      volumes:
        - name: ollama-models
          persistentVolumeClaim:
            claimName: ollama-models-pvc
      nodeSelector:
        nvidia.com/gpu.present: "true"

      #tolerations:
      #  - key: "nvidia.com/gpu"
      #    operator: "Equal"
      #    value: "present"
      #    effect: "NoSchedule"  # Ensure that the pod can be scheduled on the tainted node
